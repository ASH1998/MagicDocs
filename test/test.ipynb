{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from llama_cpp import Llama\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./server -m ~/models/zephyr-7b-beta.Q8_0.gguf -ngl 128 -c 2048 -t 16 --port 3030 --system-prompt-file /home/ashutosh/MagicDocs/magicdoc/system_prompt.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus and Neptune. '\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_completion(prompt, n_predict=1000):\n",
    "    url = 'http://localhost:3030/completion'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        'prompt': prompt,\n",
    "        'n_predict': n_predict\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return None  # You can handle error cases as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"\"\"\n",
    "###USER : Describe the below code line by line, describing each method and variable and what they do. Also describe the logic\n",
    "Code : \n",
    "def tokenize(self, text: bytes, add_bos: bool = True) -> List[int]:\n",
    "    assert self.ctx is not None\n",
    "    n_ctx = self._n_ctx\n",
    "    tokens = (llama_cpp.llama_token * n_ctx)()\n",
    "    n_tokens = llama_cpp.llama_tokenize(\n",
    "        self.ctx,\n",
    "        text,\n",
    "        tokens,\n",
    "        llama_cpp.c_int(n_ctx),\n",
    "        llama_cpp.c_bool(add_bos),\n",
    "    )\n",
    "    if n_tokens < 0:\n",
    "        n_tokens = abs(n_tokens)\n",
    "        tokens = (llama_cpp.llama_token * n_tokens)()\n",
    "        n_tokens = llama_cpp.llama_tokenize(\n",
    "            self.ctx,\n",
    "            text,\n",
    "            tokens,\n",
    "            llama_cpp.c_int(n_tokens),\n",
    "            llama_cpp.c_bool(add_bos),\n",
    "        )\n",
    "        if n_tokens < 0:\n",
    "            raise RuntimeError(\n",
    "                f'Failed to tokenize: text=\"{text}\" n_tokens={n_tokens}'\n",
    "            )\n",
    "    return list(tokens[:n_tokens])\n",
    "\n",
    "### PyDocs: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def tokenize(text: bytes, add_bos: bool = True) -> List[int]: \n",
      "    \"\"\"\n",
      "    Tokenizes the text into a list of integers.\n",
      "\n",
      "    :param text: A string to be converted to a sequence of integers (the tokens).\n",
      "    :param add_bos: Whether or not to include BOS token at the beginning.\n",
      "    :return: A list of integers representing a tokenized version of the provided text.\n",
      "    \"\"\"\n",
      "    assert self.ctx is not None\n",
      "    n_ctx = self._n_ctx\n",
      "    tokens = (llama_cpp.llama_token * n_ctx)()\n",
      "    n_tokens = llama_cpp.llama_tokenize(\n",
      "        self.ctx,\n",
      "        text,\n",
      "        tokens,\n",
      "        llama_cpp.c_int(n_ctx),\n",
      "        llama_cpp.c_bool(add_bos),\n",
      "    )\n",
      "    if n_tokens < 0:\n",
      "        n_tokens = abs(n_tokens)\n",
      "        tokens = (llama_cpp.llama_token * n_tokens)()\n",
      "        n_tokens = llama_cpp.llama_tokenize(\n",
      "            self.ctx,\n",
      "            text,\n",
      "            tokens,\n",
      "            llama_cpp.c_int(n_tokens),\n",
      "            llama_cpp.c_bool(add_bos),\n",
      "        )\n",
      "        if n_tokens < 0:\n",
      "            raise RuntimeError(\n",
      "                f'Failed to tokenize: text=\"{text}\" n_tokens={n_tokens}'\n",
      "            )\n",
      "    return list(tokens[:n_tokens])\n"
     ]
    }
   ],
   "source": [
    "resp = get_completion(a, n_predict=2000)\n",
    "print(resp['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
